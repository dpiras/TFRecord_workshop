{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop_TFrecord.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1oWMxS3btWi6"
      ],
      "authorship_tag": "ABX9TyP4NaRjvFxzTxYaJitatgat"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "58402c85a68d4f84884633147fe06bc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c25a19a540d14ab5bb7c4ab49d59efcc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_126ec236037446eba71bd03f4967ff03",
              "IPY_MODEL_c76959eed6bc48acae1c0af74fa6b568"
            ]
          }
        },
        "c25a19a540d14ab5bb7c4ab49d59efcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "126ec236037446eba71bd03f4967ff03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_460904cb55194a0fa871e2f2bf94c491",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f2ec36ec903473abc85e558579a1a44"
          }
        },
        "c76959eed6bc48acae1c0af74fa6b568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c96af04c8b104feeb9bffd3a50818f13",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:02&lt;00:00,  1.71 file/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7a2797543349437db5c939a740c9a417"
          }
        },
        "460904cb55194a0fa871e2f2bf94c491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f2ec36ec903473abc85e558579a1a44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c96af04c8b104feeb9bffd3a50818f13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7a2797543349437db5c939a740c9a417": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRpjaIlEtRjI"
      },
      "source": [
        "# Using TensorFlow with big data: the complicated case of TFRecord"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awFHf6WJ3mNa"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/drive/12bnTDGZB4Br_6AHcY3cfCtl_-5PQVY2j\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oWMxS3btWi6"
      },
      "source": [
        "## A short walk-through to give a solution to the following problem \n",
        "\n",
        "Imagine this scenario. You are trying to train a big model on some big data you were given. The solution should be easy: just plug in a neural network like the one you saw during your Machine Learning with Big Data course, load your data and feed them to your model. Wait a few hours and you're done. You leave your laptop working, and go grab some lunch. Mic drop, job done.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://raw.githubusercontent.com/dpiras/TFrecord_workshop/main/job_done.png' />\n",
        "</figure>\n",
        "\n",
        "\n",
        "Except that when you come back from your lunch break you realise your model never started training, because there is not enough memory to store all that data at once. You try a few things, but there is a fundamental issue in the amount of memory that you are requiring, and you realise you won't be able to train your model that easily.\n",
        "\n",
        "If you are lucky enough to work or study in a place that can give you extra resources, more powerful GPUs or unlimited credits on [AWS](https://aws.amazon.com/), then you might get away with it. However, even with, say, a bigger GPU, you might not be able to fit all of your data in memory, and big data centers have their own challenges: the interface might be in old, plain Linux, and the energy consumption has a [non-negligible environental impact](https://www.nature.com/articles/d41586-018-06610-y#:~:text=Data%20centres%20contribute%20around%200.3,than%202%25%20of%20global%20emissions.).\n",
        "\n",
        "`TensorFlow` provides a way out, in the form of `TFRecord`. Through this utility, you can binarise your data, thus making them easier to read, write and store, leaving an overall smaller memory footprint. However, understanding and working with `TFRecord` can be a pain, due to lack of documentation and complicated jargon.\n",
        "\n",
        "In this guide, we will try to give you an overview of the problem, as well as a practical way to make the most out of `TFRecord`, without worrying too much about the finer details. For those who are more keen, there is a list of resources at the end, which provide more insight about this niche tool, and from which we took inspiration in putting together this notebook.\n",
        "\n",
        "For any additional information feel free to send an email to Davide Piras at d.piras@ucl.ac.uk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWIpbKK9hRZm"
      },
      "source": [
        "## Let's get hands-on "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJjhIe_It9-1"
      },
      "source": [
        "### Training a CNN on MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tboVYr-hbNV"
      },
      "source": [
        "We first import some standard libraries. We will be working with `TensorFlow` (TF) 2.x, depending on which one is available on your machines.\n",
        "\n",
        "TF2 is relatively recent, and is the newer version of TF. If you have only seen TF1, do not worry! There are [plenty of examples available](https://github.com/ageron/handson-ml2) to begin this journey."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P3biYb1hYXZ",
        "outputId": "8a7523bd-ba3e-4a74-a9cd-a63a71a61aba"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "# print TF version\n",
        "print(tf.__version__)\n",
        "\n",
        "# this is to have easy access to TF datasets, for later on\n",
        "import tensorflow_datasets as tfds\n",
        "# this is an experimental feature in TF; we won't cover it here\n",
        "# it's related to the choice of how to select batches of data during training\n",
        "# in a memory efficient way\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow1xUh0T8pfb"
      },
      "source": [
        "The first goal will be downloading the MNIST dataset, the standard ML dataset containing 70000 images of labelled handwritten digits with size 28x28x1. You should already be quite familiar with it, but in case you want to read more, you can do so [here](https://en.wikipedia.org/wiki/MNIST_database)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kphnne4oJMz4"
      },
      "source": [
        "In order to download and easily deal with the pure MNIST dataset, we use a couple of non-standard libraries and commands that can do the job for us (note that there are other options available for this, and we will explore some of them below). Commands starting with `!` are bash commands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHW9ITLv0Eb7",
        "outputId": "e4e4fa08-9b9e-4d0b-e789-9e620b44529f"
      },
      "source": [
        "! pip install python-mnist"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-mnist\n",
            "  Downloading https://files.pythonhosted.org/packages/64/f0/6086b84427c3bf156ec0b3c2f9dfc1d770b35f942b9ed8a64f5229776a80/python_mnist-0.7-py2.py3-none-any.whl\n",
            "Installing collected packages: python-mnist\n",
            "Successfully installed python-mnist-0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux1CIOB6Bgsq",
        "outputId": "fb5419bb-2c21-42a9-de2e-e49d084951fe"
      },
      "source": [
        "# the fist two lines used to work, but are currently failing due to a 503 error;\n",
        "# ignore them for now, but they offer an alternative that will take a few minutes\n",
        "# to download the dataset, depending on your internet connection and colab overall usage\n",
        "#! cd ./python-mnist/\n",
        "#! ./python-mnist/bin/mnist_get_data.sh\n",
        "\n",
        "# these two commands currently work\n",
        "! wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "! tar -zxvf MNIST.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-30 10:49:39--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n",
            "--2021-04-30 10:49:39--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: â€˜MNIST.tar.gzâ€™\n",
            "\n",
            "MNIST.tar.gz            [     <=>            ]  33.20M  37.5MB/s    in 0.9s    \n",
            "\n",
            "2021-04-30 10:49:40 (37.5 MB/s) - â€˜MNIST.tar.gzâ€™ saved [34813078]\n",
            "\n",
            "MNIST/\n",
            "MNIST/raw/\n",
            "MNIST/raw/train-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "MNIST/raw/train-images-idx3-ubyte\n",
            "MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-images-idx3-ubyte\n",
            "MNIST/raw/train-images-idx3-ubyte.gz\n",
            "MNIST/processed/\n",
            "MNIST/processed/training.pt\n",
            "MNIST/processed/test.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqEQLHX_JvYM"
      },
      "source": [
        "We downloaded the MNIST dataset locally. It is good practice to explore it before playing with it, but we assume you are familiar with this dataset already. \n",
        "\n",
        "Out of curiosity, we want to print the size of the folder containing the dataset. Note that the folder contains also some other files, which we do not want to include in our calculation. Hint: there is a bash command to find the size of a folder and its files, and this is `du`. To explore the suggested solution, you can double click on the form. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mppaeRAuicn3"
      },
      "source": [
        "# YOUR ANSWER HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S3fCXUdDs5Y",
        "cellView": "form"
      },
      "source": [
        "#@title Solution\n",
        "# -s stands for summarise\n",
        "# -h stands for human-readable\n",
        "# -c prints the sum as well\n",
        "! du -shc MNIST/raw/*ubyte"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrNWsnubKCUc"
      },
      "source": [
        "We collect training and testing images and labels, and execute a few operations for them to be ready to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIP5QWqjFzI7"
      },
      "source": [
        "from mnist import MNIST\n",
        "mndata = MNIST('./MNIST/raw')\n",
        "images_train, labels_train = mndata.load_training()\n",
        "images_test, labels_test = mndata.load_testing()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLfgrCqmFzFp"
      },
      "source": [
        "images_train = np.array(images_train).reshape(-1, 28, 28, 1)\n",
        "labels_train = np.array(labels_train)\n",
        "images_test = np.array(images_test).reshape(-1, 28, 28, 1)\n",
        "labels_test = np.array(labels_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "D25xErkQFzBx",
        "outputId": "aba70c62-0c55-440d-91b3-1a4bc7372b9a"
      },
      "source": [
        "# we have a look at the first image; it's not good practice to break a for loop\n",
        "# but let's just ignore it in this case, as we will compare it to TFRecord\n",
        "for image, label in zip(images_train, labels_train):\n",
        "  print(image.shape)\n",
        "  plt.imshow(image[:, :, 0], cmap='gray')\n",
        "  plt.title(f'Label: {label}', fontsize=18)\n",
        "  plt.colorbar()\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGLWYWmkKelX"
      },
      "source": [
        "We then define a general model for classifying the digits in the dataset. This is the standard example of supervised learning, and you can refer to the lecture material for more information. Note that we use 2D convolutional layers, which should give us a better performance in terms of accuracy, but this is not essential for `TFRecord`, of course. Can you describe/sketch the neural network we are using for this classification task?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyRGinEuFy9y"
      },
      "source": [
        "def get_cnn():\n",
        "  model = tf.keras.Sequential([\n",
        "      \n",
        "    tf.keras.layers.Conv2D(kernel_size=3, filters=16, padding='same', activation='relu', input_shape=[28,28, 1]),\n",
        "    tf.keras.layers.Conv2D(kernel_size=3, filters=32, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(kernel_size=3, filters=64, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(kernel_size=3, filters=128, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(kernel_size=3, filters=256, padding='same', activation='relu'),\n",
        "    \n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(10)\n",
        "  ])\n",
        "\n",
        "  model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oM5729cFy3C",
        "outputId": "4514268c-794a-414c-9c85-eeb1ff1cfa2a"
      },
      "source": [
        "model = get_cnn()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 16)        160       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 3, 3, 256)         295168    \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 394,890\n",
            "Trainable params: 394,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G2j5VxvILN1"
      },
      "source": [
        "# a few hyperparameters; feel free to play with them\n",
        "batch_size = 128\n",
        "epochs = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POnq9KA8FyrN",
        "outputId": "d78ea14d-11df-41ae-c5ae-40ff50179bff"
      },
      "source": [
        "# we fit our model and look at the performance on the test set as we progress through the training\n",
        "# note we also take note of the total time to train the model\n",
        "start = time.time()\n",
        "model.fit(\n",
        "    x = images_train, \n",
        "    y = labels_train,\n",
        "    validation_data=(images_test, labels_test),\n",
        "    epochs=epochs,\n",
        ")\n",
        "print(f'Total time to fit the model was {((time.time()-start))/60:.2f} minutes')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 23s 4ms/step - loss: 0.6090 - sparse_categorical_accuracy: 0.8486 - val_loss: 0.0411 - val_sparse_categorical_accuracy: 0.9868\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0503 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.0392 - val_sparse_categorical_accuracy: 0.9872\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0327 - sparse_categorical_accuracy: 0.9895 - val_loss: 0.0280 - val_sparse_categorical_accuracy: 0.9913\n",
            "Total time to fit the model was 0.60 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkRwK-BPLcap"
      },
      "source": [
        "Each epoch should take about 190 s, if not using any specific hardware accelerator. With a standard GPU, it took about 10 seconds/epoch - we recommend using one, to speed things up! \n",
        "\n",
        "In order to do so, you can simply click on `Runtime`, then `Change runtime type`, and choose your favourite one. Note though that on `colab` it is good practice to not use resources you do not really need, and the chances of being suspended for bad use of resources are quite high - trust us!\n",
        "\n",
        "With a \"high-ram\" GPU, it takes roughly the same, while with a 'high-ram' TPU it took a bit more, but this will not be covered here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgSC7cVpWybG"
      },
      "source": [
        "Now, you might be happy - and you should be - as in no more than 10 minutes without GPU you got a 1% error rate on the test set. However, there are many cases, as explained above, where you need to allocate your memory in a more calibrated way, sometimes to avoid catastrophic failures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuRuL5ynZ6rl"
      },
      "source": [
        "### Training a CNN on MNIST using `TFRecord`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O581-NPGaAEx"
      },
      "source": [
        "We download the official TF version of the MNIST dataset using `TensorFlow` itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GamxSYBhZ_5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202,
          "referenced_widgets": [
            "58402c85a68d4f84884633147fe06bc8",
            "c25a19a540d14ab5bb7c4ab49d59efcc",
            "126ec236037446eba71bd03f4967ff03",
            "c76959eed6bc48acae1c0af74fa6b568",
            "460904cb55194a0fa871e2f2bf94c491",
            "8f2ec36ec903473abc85e558579a1a44",
            "c96af04c8b104feeb9bffd3a50818f13",
            "7a2797543349437db5c939a740c9a417"
          ]
        },
        "outputId": "1f6371ae-6c56-4202-a0db-5cafb837d1bd"
      },
      "source": [
        "(ds_train, ds_test), ds_info = tfds.load('mnist', split=['train', 'test'], shuffle_files=True, as_supervised=True, with_info=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
            "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58402c85a68d4f84884633147fe06bc8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=4.0, style=ProgressStyle(descriptioâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-dLuJJA9ff5"
      },
      "source": [
        "As you can see, the whole dataset, which was 53 MB, was downloaded to a folder on Colab. The dataset is stored already as `TFRecord` files. Let's explore it a bit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_6WgDyM40vI",
        "outputId": "55b2e601-0e7f-42ca-e696-c35ba526054d"
      },
      "source": [
        "ds_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_OptionsDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jphMuDoxkV4R",
        "outputId": "b1e7adb4-a8eb-4ca9-e80a-230bb40076ee"
      },
      "source": [
        "# look at this only after you've answered the first question above! \n",
        "! du -shc ../root/tensorflow_datasets/mnist/3.0.1/mnist*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.2M\t../root/tensorflow_datasets/mnist/3.0.1/mnist-test.tfrecord-00000-of-00001\n",
            "19M\t../root/tensorflow_datasets/mnist/3.0.1/mnist-train.tfrecord-00000-of-00001\n",
            "23M\ttotal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K00WO1vGk7jM"
      },
      "source": [
        "Taking inspiration from the code above, can you write a small `for` loop to show the first image and label in the training set? Note that both images and labels are stored in `ds_train`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_KQDV7olVe3"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "458-Upse7PMJ",
        "cellView": "form"
      },
      "source": [
        "#@title Solution\n",
        "for image, label in ds_train:\n",
        "  print(image.numpy().shape)\n",
        "  plt.imshow(image[:, :, 0], cmap='gray')\n",
        "  plt.title(f'Label: {label.numpy()}', fontsize=18)\n",
        "  plt.colorbar()\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSB9vQbFcZ_1"
      },
      "source": [
        "The dataset is stored in a more criptic way: note, however, that it takes only 23 MB - less than half the previous case! One advanced thing to note, though, is that each pixel is stored as an `np.uint8` data type in this case, while it was a `np.int64` previously. This reduces the precision with which each pixel is stored, but it is not a problem in this case as all pixels can just take values between 0 and 255. Clearly, a lower precision number requires less memory to be stored!\n",
        "\n",
        "To prepare this dataset for training, we need a few more steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXTVoJDHkuci"
      },
      "source": [
        "# this is 60000 for our dataset\n",
        "training_set_size = ds_info.splits['train'].num_examples "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBEA6DpIsxkF"
      },
      "source": [
        "# cache will improved speed when iterating over a dataset more than once\n",
        "# e.g. after the first epoch\n",
        "ds_train = ds_train.cache()\n",
        "# we only shuffle the training set\n",
        "ds_train = ds_train.shuffle(training_set_size)\n",
        "# we tell TF that we will take data in batches\n",
        "ds_train = ds_train.batch(batch_size)\n",
        "# this last command is used for the following:\n",
        "# \"This allows later elements to be prepared while the current element is being processed. \n",
        "# This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\"\n",
        "ds_train = ds_train.prefetch(AUTOTUNE)\n",
        "\n",
        "# and we do the same for the test set\n",
        "ds_test = ds_test.batch(batch_size)\n",
        "ds_test = ds_test.cache()\n",
        "ds_test = ds_test.prefetch(AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9RUwGJmcmN-"
      },
      "source": [
        "Then we are ready to train, just as above!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ParaEldFtX5J"
      },
      "source": [
        "model = get_cnn()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coVr60s8tX7e",
        "outputId": "4473ad4e-91c9-4ede-fd29-6b6074681a6a"
      },
      "source": [
        "start = time.time()\n",
        "model.fit(\n",
        "    ds_train,\n",
        "    validation_data=ds_test,\n",
        "    epochs=epochs,\n",
        ")\n",
        "print(f'Total time to fit the model was {((time.time()-start))/60:.2f} minutes')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "469/469 [==============================] - 8s 8ms/step - loss: 2.1295 - sparse_categorical_accuracy: 0.6847 - val_loss: 0.0581 - val_sparse_categorical_accuracy: 0.9833\n",
            "Epoch 2/3\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0630 - sparse_categorical_accuracy: 0.9806 - val_loss: 0.0375 - val_sparse_categorical_accuracy: 0.9872\n",
            "Epoch 3/3\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0392 - sparse_categorical_accuracy: 0.9877 - val_loss: 0.0436 - val_sparse_categorical_accuracy: 0.9852\n",
            "Total time to fit the model was 0.23 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBul-xtM1gfl"
      },
      "source": [
        "This should take about 170 s per epoch without hardware acceleration, and 3 s per epoch with a standard GPU. To recap: we used a ready-made `TFRecord` file that contains the MNIST dataset, and adapted it to train our model. We obtained a significant reduction in the size of the dataset, and only a marginal improvement in the speed, at the cost of a more complicated set of preprocessing steps. The accuracy does not seem to be significantly affected, as it should be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGEpqJg7tzVI"
      },
      "source": [
        "### The concept behind `TFRecord`, and how to use it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mhKv8GP4-Rs"
      },
      "source": [
        "Now we are going to do the same exercise, but this time we will create our own `TFRecord`. To show how to make your own `TFRecord` out of a dataset you have available (not necessarily MNIST), we will save it, and then re-load it from disk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDGWHKhxqXJ4"
      },
      "source": [
        "Remember that when downloading the dataset from `tfds`, it is stored as a `.tfrecord` file, with a total size of about 23 MB. This is already less than the original dataset, which is about 53 MB, as mentioned above, even though we should also keep in mind that the precision in the latter case is lower, so that everything is stored more efficiently ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onkVZDgRq4Rg"
      },
      "source": [
        "We define a few functions that are needed to convert our dataset into a `TFRecord`. To give you an idea of what we are trying to do, instead of having some `.txt`, `.csv` or `.npy`* files scattered around and that we have to load in memory every time during our model training, forcing the disks to jump between blocks, we simply store the data in a sequential layout. We can visualize this concept in the following way:\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://raw.githubusercontent.com/dpiras/TFrecord_workshop/main/TFRecord_1.png' />\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBagr7sssa5W"
      },
      "source": [
        "Every single data sample is called an `Example`, and is essentially a dictionary, storing the mapping between a key and our actual data - you should be able to grasp the concept if you are familiar with python dictionaries.\n",
        "\n",
        "An additional complication comes when you want to write your data to a `TFRecord`, as you first have to convert your data to a `Feature`. These features are then the inner components of one `Example`:\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://raw.githubusercontent.com/dpiras/TFrecord_workshop/main/TFRecord_2.png' />\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvRgzo7ImRdp"
      },
      "source": [
        "\n",
        "*`.npy` file are actually serialised files as well, and optimised for NumPy. If possible, try to use them! They occupy much less space and are faster to read/write using `numpy`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecWaD0-HuHkv"
      },
      "source": [
        "Here we start by defining the mapping dictionary for our records.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp1teyCphzbI"
      },
      "source": [
        "# the first three functions help distinguish between floats, ints\n",
        "# or bytes/strings. We save that as a Feature\n",
        "def _bytes_feature(value):\n",
        "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "    # note we consider the numpy version of the array, so we are indeed\n",
        "    # saving a numpy array, like the one you could have available\n",
        "    if isinstance(value, type(tf.constant(0))):\n",
        "        value = value.numpy()\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _float_feature(value):\n",
        "  \"\"\"Returns a floast_list from a float / double.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "def _int64_feature(value):\n",
        "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "\n",
        "# this is the function that actually serializes the array\n",
        "# where the array could be an image, some text, or even an audio file\n",
        "def serialize_array(array):\n",
        "  array = tf.io.serialize_tensor(array)\n",
        "  return array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zENf3AikiTge",
        "outputId": "430be465-ae45-421b-8bbe-1cb2e67b373a"
      },
      "source": [
        "ds_splits = [\"train\", \"test\"]\n",
        "\n",
        "# we have a for loop for each subset, i.e. for training and testing data\n",
        "# we could even split our training data in multiple files, to avoid having one bigger file\n",
        "# details on how to do this are in the resources at the end\n",
        "for d in ds_splits:\n",
        "  print(\"Saving {}\".format(d))\n",
        "  \n",
        "  # there is a small catch here: since we want to compare the results with the TFRecord case,\n",
        "  # we cast each pixel from np.int64 to np.uint8. Note that this reduces the memory requirements a bit.\n",
        "  # There is a comment about this in the solutions below\n",
        "  if d == 'train':\n",
        "    subset = zip(np.uint8(images_train), labels_train)\n",
        "  elif d == 'test':\n",
        "    subset = zip(np.uint8(images_test), labels_test)\n",
        "  else:\n",
        "    raise ValueError('The split should be either \"train\" or \"test\"')\n",
        "\n",
        "  filename = d+\".tfrecords\"\n",
        "  \n",
        "  # this is crucial: we define a Writer for the TFRecord, which we will use to create our TFRecord dataset\n",
        "  # note that we can specify different compression types, either none (''), 'GZIP' or 'ZLIB'\n",
        "  writer = tf.io.TFRecordWriter(filename, options=tf.io.TFRecordOptions(compression_type=''))\n",
        "  count = 0\n",
        "  for image, label in subset:\n",
        "    # we need to specify the details of our images\n",
        "    data={\n",
        "        'height': _int64_feature(28),\n",
        "        'width': _int64_feature(28),\n",
        "        'depth': _int64_feature(1),\n",
        "        'label': _int64_feature(label),\n",
        "        'image_raw':_bytes_feature(serialize_array(image))\n",
        "        }\n",
        "        \n",
        "    out = tf.train.Example(features=tf.train.Features(feature=data))\n",
        "    writer.write(out.SerializeToString())\n",
        "    count +=1\n",
        "  writer.close()\n",
        "  print(count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving train\n",
            "60000\n",
            "Saving test\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUfE69CpupEU"
      },
      "source": [
        " Note that our images are made of integers, and every pixel goes from 0 to 255. If we wanted to normalise our data and consider each pixel as a float, we would need to change our dictionary, of course! Note that we would also need to change `parse_tensor` below to use `tf.float64`, which would require more memory. We won't be covering this here, but feel free to try it for yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08lxKx1rvR8X"
      },
      "source": [
        "We can check the size of our TFRecord by using the same command as above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0X9K8i0MkKnv",
        "outputId": "9824104e-0cef-4758-f4c4-fb72e18ef460"
      },
      "source": [
        "! du -shc ./*.tfrecords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.7M\t./test.tfrecords\n",
            "53M\t./train.tfrecords\n",
            "61M\ttotal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TvYXWj4q3BG"
      },
      "source": [
        "What do you observe? What changes when you change the compression type?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kQfuZ2FsRUn"
      },
      "source": [
        "# YOUR ANSWER HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdN81YoCsYj4",
        "cellView": "form"
      },
      "source": [
        "#@title Solution\n",
        "print('When not compressing the TFRecord, we obtain similar or bigger sizes as in the first case: this is because TFRecord _per se_ is not efficient at storing data. \\nHowever, these files can be easily compressed, getting to as little as 12 MB! \\nStill, note that the same compression type will have to be specified below, and in principle compression could slow loading and training down a bit. \\nA more advanced note: if we had kept the np.int64 data type for the pixels, we would be using ~300 MB without compression, and ~19 MB with compression \\nThis would still be better than both the given TFRecord file and the numpy array!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu51XRd3wBlv"
      },
      "source": [
        "We now focus our attention on reading back the `TFRecord` files, and load them back into memory. Note we need to specify a dictionary again, to explain to `TensorFlow` how to \"read\" the data and unserialise them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d_iiWhNiorq"
      },
      "source": [
        "def parse_tfr_elem(element):\n",
        "  # this is the dictionary needed to read the data back in\n",
        "  # note each image is read back in as a string\n",
        "  parse_dict = {\n",
        "      'height': tf.io.FixedLenFeature([], tf.int64),\n",
        "      'width':tf.io.FixedLenFeature([], tf.int64),\n",
        "      'label':tf.io.FixedLenFeature([], tf.int64),\n",
        "      'depth':tf.io.FixedLenFeature([], tf.int64),\n",
        "      'image_raw': tf.io.FixedLenFeature([], tf.string)\n",
        "  }\n",
        "  \n",
        "  example_message = tf.io.parse_single_example(element, parse_dict)\n",
        "\n",
        "  # we read each image in the TFRecord using this dictionary\n",
        "  img_raw = example_message['image_raw']\n",
        "  height = example_message['height']\n",
        "  width = example_message['width']\n",
        "  depth = example_message['depth']\n",
        "  label = example_message['label']\n",
        "\n",
        "  feature = tf.io.parse_tensor(img_raw, out_type=tf.uint8)\n",
        "  feature = tf.reshape(feature, shape=[height,width,depth])\n",
        "  return (feature, label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0NPh6aGi6ip"
      },
      "source": [
        "def get_dataset(filename, set_type):\n",
        "    # you can ignore these first two lines\n",
        "    ignore_order = tf.data.Options()\n",
        "    ignore_order.experimental_deterministic = False  # disable native order, increase speed\n",
        "\n",
        "    # it is very important here to specify the same compression type as above\n",
        "    # otherwise TF won't be able to read the data back in!\n",
        "    dataset = tf.data.TFRecordDataset(filename, compression_type='')\n",
        "    \n",
        "    dataset = dataset.with_options(\n",
        "        ignore_order\n",
        "    )  \n",
        "\n",
        "    # now, when we read back into memory, we explain TF how to read each element\n",
        "    # 'map' is used to apply a function - in this case, parse_tfr_elem - to all the elements in dataset\n",
        "    dataset = dataset.map(\n",
        "        parse_tfr_elem, num_parallel_calls=AUTOTUNE\n",
        "    )\n",
        "    \n",
        "    # now we can repeat the steps as above\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.shuffle(training_set_size)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(AUTOTUNE)\n",
        "    # the next command is needed to re-use the training set\n",
        "    # note that if we do this, we also need to specify\n",
        "    # steps_per_epoch=training_set_size//batch_size when calling model.fit!\n",
        "    # the results are equivalent\n",
        "    #dataset = dataset.repeat() if set_type =='train' else dataset \n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUKiMsZRw2QL"
      },
      "source": [
        "With the two functions defined above, it is now easy to load the saved `TFRecord` files, which are ready to be fed into our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taITgSffjBrC"
      },
      "source": [
        "tfr_dataset_train = get_dataset('train.tfrecords', 'train')\n",
        "tfr_dataset_test = get_dataset('test.tfrecords', 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yra6FbYUiUgp"
      },
      "source": [
        "model = get_cnn()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCuJBqjsqR6k"
      },
      "source": [
        "Can you write the final cell to fit the model on the dataset just created? Hint: you need to specify `steps_per_epoch` in addition to what was done above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPqjyTSmiUja",
        "cellView": "form"
      },
      "source": [
        "#@title Solution\n",
        "start = time.time()\n",
        "model.fit(tfr_dataset_train, epochs=epochs, validation_data=tfr_dataset_test)\n",
        "print(f'Total time to fit the model with TFRecord was {((time.time()-start))/60:.2f} minutes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDpWuckCdtRt"
      },
      "source": [
        "You should get similar times as the second experiment above: all in all we are using the same methodology, and we just re-built our `TFRecord`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAkTt9mHwxDI"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaGYsKf0wzNI"
      },
      "source": [
        "This notebook provides a hands-on solution for cases that happen often in real life: we have limited resources, and we need to carefully allocate memory, to store the data and train the model. As you could see, in this case the advantage was only marginal, hence `TFRecord` is not the solution to all problems, and it should be carefully designed and tested before spending too much time optimising it. In addition to that, you could observe how complicated things can get when we need to save a binary file, and the theory behind it is well beyond the scope of this short workshop. Feel free to draw more inspiration from the links posted below!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFryvMd_wvZE"
      },
      "source": [
        "## Sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTSZKundzNYK"
      },
      "source": [
        "[Official TFRecord documentation](https://www.tensorflow.org/tutorials/load_data/tfrecord)\n",
        "\n",
        "[Tensorflow Records? What they are and how to use them](https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564)\n",
        "\n",
        "[TFRecords Explained](https://towardsdatascience.com/tfrecords-explained-24b8f2133282)\n",
        "\n",
        "[A practical guide to TFRecords](https://towardsdatascience.com/a-practical-guide-to-tfrecords-584536bc786c)\n",
        "\n",
        "[Using TFRecords to Train a CNN on MNIST](https://towardsdatascience.com/using-tfrecords-to-train-a-cnn-on-mnist-aec141d65e3d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYOHKTytgZW7"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://raw.githubusercontent.com/dpiras/TFrecord_workshop/main/psst.jpg' />\n",
        "</figure>"
      ]
    }
  ]
}